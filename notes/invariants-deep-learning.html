<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8">
    <title>On Invariants and the Power of Deep Learning Architectures</title>
    <link rel="stylesheet" href="../css/basic.css">
</head>

<body>
    <h1>On Invariants and the Power of Deep Learning Architectures</h1>

    <p class="info">
        September 2020
    </p>

    Stéphane Mallat, introductory lecture at Collège de France (2018) asks: what invariants do we learn from the ImageNet classification task? (Taking the point of view of Galois that symmetry is at the core of studying complex systems.)
    
    Logical approach: axiomatization of algebraic transformations that define invariants (e.g. translation, on which Mallet insists in his lecture). Then projection from linear spaces to logical axioms (or, more precisely the data structures that encode assertions).
    
    Relates to the axiomatic learning of curves (see note).
</body>
<html>
