<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8">
    <title>Comments on "Mind Over Machine"</title>
    <link rel="stylesheet" href="../css/basic.css">
</head>

<body>
    <h1>Comments on "Mind Over Machine"</h1>

    <p class="info">
        August 2020
    </p>

    Hubert L. Dreyfus and Stuart E. Dreyfus, Mind over Machine, 1986.

    The point of the book is clear: human expertise cannot be captured as facts and rules (that computers could then follow, at the basis of AI).
    More precisely, experts are not capable of <em>expressing</em> their knowledge as facts and rules.
    The single evidence for that statement is a five-step model of learning, the last steps being based on global pattern recognition instead of deductive reasoning.

    Formal comment: the kind of facts and rules is never made fully explicit in the book: propositional logic, Datalog, FOL?
    Then, digression about utility theory, which can be axiomatized but is generally not seen as a purely logical framework.
    Let's assume Datalog with a built-in to create Skolem objects.

    The point could have been made more simply by insisting that practice is as important as theory in learning skills.
    Before the 80s, only few works on reinforcement learning, mostly due to hardware limitations.
    But with ML, computer scientists have taken precisely the opposite view of only relying on experience to learn representations.

    Pre-defined (axiomatized) theory generally plays the role of accelerators in learning.
    Assumption: practice is nonetheless needed not to "bend the rules" but to <em>interpret</em> them correctly.
    Practice, then, solves the problem of denotation (of mapping symbols to physical world objects or, <i>a minima</i>, to perception patterns).
    ML researchers have considered using pre-defined theories to reduce the size of the overall representational space of a theory.
    But what is also possible is to project numeric representations generated by ML into symbolic spaces, using facts and rules,
    which is a probable trend towards "explainable AI". After reading Mind over Machine, one could easily consider "explainable AI" as an oxymoron.
    
    Yet, two comments to keep faith.

    First comment: if faced with expressivity problems, experts invent new terminologies to exchange.
    For instance, they would give names to specific patterns they can recognize (such as 'smells' in software engineering).
    So, it is not obvious facts and rules cannot represent expertise if the terminology is extensible.

    Second comment: that experts are not always capable of introspection is not entirely surprising.
    We cannot only think of our own thinking to a certain extent (Minsky treats the subject in "The Emotion Machine").
    Still, an external observer may still be capable of inferring rules (or any model that can be projected onto rules)
    from an expert's decisions.
    One of the last examples of the Dreyfus brothers (whatever images of mass extinction that example invokes):
    sorting chicks according to their sex after one day of existence. Humans seem good at the exercise (99%+ accuracy)
    but not concerns at expressing how they do it. At some point, human experts didn't know where to look/touch
    to figure out the sex of the animal, so if we had a sentient machine that could obtain an build a representation of the animal,
    there are numerous supervised algorithms to optimize structured representations.
</body>
<html>
